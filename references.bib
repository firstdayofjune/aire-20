% Encoding: UTF-8
@article{ribeiro_why_2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	journaltitle = {{arXiv}:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2020-02-28},
	date = {2016-08-09},
	year = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.04938},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/A6SNTW88/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf}
}

@inproceedings{murukannaiah_acquiring_2016,
	title = {Acquiring Creative Requirements from the Crowd: Understanding the Influences of Personality and Creative Potential in Crowd {RE}},
	doi = {10.1109/RE.2016.68},
	shorttitle = {Acquiring Creative Requirements from the Crowd},
	abstract = {As a creative discipline, Requirements Engineering ({RE}), lends importance to understanding the associated human factors. Crowd {RE}, the approach of acquiring requirements from members of the public-the so-called crowd-emphasizes human factors further. We investigate how human personality and creative potential influence a requirement acquisition task. These factors are of specific importance to Crowd {RE} because (1) crowd workers are generally not trained in {RE}, and (2) a key motivation in engaging them is to benefit from their creativity. We propose a sequential Crowd {RE} process, where workers in one stage review requirements from the previous stage and produce additional requirements. To reduce potential information overload in this process, we propose strategies for selecting requirements from one stage to expose to workers in later stages. We conducted a study on Amazon Mechanical Turk tasking 300 workers with creating requirements via the above sequential process (in the domain of smart home applications for concreteness) and tasking an additional 300 workers to rate the creativity (novelty and usefulness) of those requirements. Our findings offer insights on how to carry out Crowd {RE} effectively. First, we find that a crowd worker's (1) creative potential, and personality traits of openness and conscientiousness have significant positive influence on the novelty of the worker's ideas, and (2) personality traits of agreeableness and conscientiousness have significant positive influence, but extraversion has significant negative influence on the usefulness of the worker's ideas. Second, we find that exposing a worker to ideas from previous workers cognitively stimulates the worker to produce creative ideas. Third, we identify effective strategies based on personality traits and creative potential for selecting a few requirements from a pool of previous requirements to stimulate a worker.},
	eventtitle = {2016 {IEEE} 24th International Requirements Engineering Conference ({RE})},
	pages = {176--185},
	booktitle = {2016 {IEEE} 24th International Requirements Engineering Conference ({RE})},
	author = {Murukannaiah, Pradeep K. and Ajmeri, Nirav and Singh, Munindar P.},
	date = {2016-09},
	year = {2016},
	keywords = {agreeableness, Amazon Mechanical Turk, conscientiousness, creative potential, creative requirements, creativity, Creativity, Crowd {RE}, crowdsourcing, Crowdsourcing, extraversion, formal specification, formal verification, human factors, Human factors, human personality, Idea generation, openness, personality, Positron emission tomography, requirement acquisition, requirements engineering, Requirements engineering, sequential crowd {RE} process, smart home, Smart homes, Stakeholders},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/83VESVA2/7765523.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/28VJ36FF/Murukannaiah et al. - 2016 - Acquiring Creative Requirements from the Crowd Un.pdf:application/pdf}
}

@inproceedings{mavin_towards_2019,
	title = {Towards an Ontology of Requirements Engineering approaches},
	doi = {10.1109/RE.2019.00080},
	abstract = {Requirements are a key factor in determining the success or failure of the system development process. Requirements engineering is a creative problem-solving process whose primary purpose is to enable researchers and practitioners to apply appropriate theories, models, techniques and tools to understand and support the requirements processes more effectively. However, there is a multitude of ways to conduct the requirements engineering process and the quality of the requirements can be greatly influenced by the approaches employed. While consensus exists that no one approach works in all situations, how do practitioners and researchers select the most relevant and appropriate approach(es)? In order to understand this, we argue that a community-based effort is required to organise the plethora of requirements engineering approaches into an ontology. Such a structure would provide an opportunity to identify gaps and to improve the interfaces between approaches. Crowdsourcing the development and validation of such an ontology would facilitate its application across different system types and application domains. Keywords: Requirements Engineering, Community-based Requirements Engineering, Crowdsourcing, Ontology, Methods, Tools, Techniques, Approaches.},
	eventtitle = {International Requirements Engineering Conference ({RE})},
	pages = {514--515},
	author = {Mavin, Alistair and Mavin, Sabine and Penzenstadler, Birgit and Venters, Colin C.},
	year = {2019},
	langid = {english},
	file = {Towards an Ontology of Requirements Engineering Approaches.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/M8884KXM/Towards an Ontology of Requirements Engineering Approaches.pdf:application/pdf}
}

@inproceedings{mohagheghi_what_2017,
	title = {What Contributes to the Success of {IT} Projects? Success Factors, Challenges and Lessons Learned from an Empirical Study of Software Projects in the Norwegian Public Sector},
	doi = {10.1109/ICSE-C.2017.146},
	shorttitle = {What Contributes to the Success of {IT} Projects?},
	abstract = {Context. Each year the public sector invests large amounts of money in the development and modifications of their software systems. These investments are not always successful and many public sector software projects fail to deliver the expected benefits. Goal. This study aims at reducing the waste of resources on failed software projects through better understanding of the success factors and challenges. Method. Thirty-five completed software projects in 11 organizations in the public sector of Norway were analyzed. For each project, representatives from the project owners, project management and the user organization were interviewed. Results. Small and large software projects reported different challenges, especially related to project priority. Taking advantage of agile practices such as flexible scope and frequent delivery increased the success rate of the projects. Projects with time and material contracts and involved clients during execution were more successful than other projects. The respondents experienced that extensive involvement and good competence of the client, high priority of the project, good dialogue between client and provider and appliance of agile practices were main success factors. Main challenges were related to technical issues, project planning and management, transition of the product to the user organization, involvement and competence of the client, and benefit management. Conclusions. Success factors tend to focus on human factors, e.g., involvement, competence and collaboration. Challenges focus on human factors as well as issues of technical nature. Both aspects need to be addressed to enable successful and avoid failed software projects. Competence, client involvement and benefit management are among factors that the public sector should focus on for realizing client benefits.},
	eventtitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	pages = {371--373},
	booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	author = {Mohagheghi, Parastoo and Jørgensen, Magne},
	date = {2017-05},
	year = {2017},
	keywords = {agile, agile practices, benefit management, client competence, client involvement, client-provider dialogue, contract type, Contracts, empirical study, extensive involvement, human factors, Investment, {IT} projects, Norway, Norwegian public sector, organisational aspects, Organizations, Planning, product transition, project management, Project management, project material contracts, project owners, project planning, project priority, Software, software management, software project empirical study, software projects, software prototyping, software systems, Stakeholders, success, success factors, technical issues, user organization},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/WYXI5ZL3/7965362.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/KY5BD2V9/Mohagheghi and Jørgensen - 2017 - What Contributes to the Success of IT Projects Su.pdf:application/pdf}
}

@inproceedings{stanik_classifying_2019,
	title = {Classifying Multilingual User Feedback using Traditional Machine Learning and Deep Learning},
	doi = {10.1109/REW.2019.00046},
	abstract = {With the rise of social media like Twitter and of software distribution platforms like app stores, users got various ways to express their opinion about software products. Popular software vendors get user feedback thousandfold per day. Research has shown that such feedback contains valuable information for software development teams such as problem reports or feature and support inquires. Since the manual analysis of user feedback is cumbersome and hard to manage many researchers and tool vendors suggested to use automated analyses based on traditional supervised machine learning approaches. In this work, we compare the results of traditional machine learning and deep learning in classifying user feedback in English and Italian into problem reports, inquiries, and irrelevant. Our results show that using traditional machine learning, we can still achieve comparable results to deep learning, although we collected thousands of labels.},
	eventtitle = {2019 {IEEE} 27th International Requirements Engineering Conference Workshops ({REW})},
	pages = {220--226},
	booktitle = {2019 {IEEE} 27th International Requirements Engineering Conference Workshops ({REW})},
	author = {Stanik, Christoph and Haering, Marlo and Maalej, Walid},
	date = {2019-09},
	year = {2019},

	keywords = {Data-Driven Requirements, Data Mining, Social Media Analytics, Machine Learning, Deep Learning, deep learning, {DP} industry, multilingual user feedback, pattern classification, social media, social networking (online), software development teams, software distribution platforms, software products, software vendors, supervised learning, supervised machine learning approaches, tool vendors, Twitter, user feedback},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/PZ8IDSB3/8933719.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/GR5QHGFL/Stanik et al. - 2019 - Classifying Multilingual User Feedback using Tradi.pdf:application/pdf}
}

@inproceedings{oriol_fame_2018,
	title = {{FAME}: Supporting Continuous Requirements Elicitation by Combining User Feedback and Monitoring},
	doi = {10.1109/RE.2018.00030},
	shorttitle = {{FAME}},
	abstract = {Context: Software evolution ensures that software systems in use stay up to date and provide value for end-users. However, it is challenging for requirements engineers to continuously elicit needs for systems used by heterogeneous end-users who are out of organisational reach. Objective: We aim at supporting continuous requirements elicitation by combining user feedback and usage monitoring. Online feedback mechanisms enable end-users to remotely communicate problems, experiences, and opinions, while monitoring provides valuable information about runtime events. It is argued that bringing both information sources together can help requirements engineers to understand end-user needs better. Method/Tool: We present {FAME}, a framework for the combined and simultaneous collection of feedback and monitoring data in web and mobile contexts to support continuous requirements elicitation. In addition to a detailed discussion of our technical solution, we present the first evidence that {FAME} can be successfully introduced in real-world contexts. Therefore, we deployed {FAME} in a web application of a German small and medium-sized enterprise ({SME}) to collect user feedback and usage data. Results/Conclusion: Our results suggest that {FAME} not only can be successfully used in industrial environments but that bringing feedback and monitoring data together helps the {SME} to improve their understanding of end-user needs, ultimately supporting continuous requirements elicitation.},
	eventtitle = {2018 {IEEE} 26th International Requirements Engineering Conference ({RE})},
	pages = {217--227},
	booktitle = {2018 {IEEE} 26th International Requirements Engineering Conference ({RE})},
	author = {Oriol, Marc and Stade, Melanie and Fotrousi, Farnaz and Nadal, Sergi and Varga, Jovan and Seyff, Norbert and Abello, Alberto and Franch, Xavier and Marco, Jordi and Schmidt, Oleg},
	date = {2018-08},
	year = {2018},
	note = {{ISSN}: 1090-705X},
	keywords = {Conferences, continuous requirements elicitation, Data acquisition, {FAME}, feedback, Feedback gathering, usage monitoring, requirements, user involvement, feedback acquisition, data collection, requirements elicitation, software evolution, user feedback, formal specification, German small and medium-sized enterprise, heterogeneous end-users, industrial environments, information sources, Internet, Memory, mobile contexts, monitoring, Monitoring, monitoring data, online feedback mechanisms, Ontologies, organisational aspects, organisational reach, personal computing, Requirements engineering, requirements engineers, small-to-medium enterprises, {SME}, software evolution, software systems, systems analysis, Tools, user feedback, Web application},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/6GF2NR7B/8491137.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/FCDQW2NB/Oriol et al. - 2018 - FAME Supporting Continuous Requirements Elicitati.pdf:application/pdf}
}

@inproceedings{palomba_user_2015,
	title = {User reviews matter! Tracking crowdsourced reviews to support evolution of successful apps},
	doi = {10.1109/ICSM.2015.7332475},
	abstract = {Nowadays software applications, and especially mobile apps, undergo frequent release updates through app stores. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we show-by performing a study on 100 Android apps-how developers addressing user reviews increase their app's success in terms of ratings. Specifically, we devise an approach, named {CRISTAL}, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results indicate that developers implementing user reviews are rewarded in terms of ratings. This poses the need for specialized recommendation systems aimed at analyzing informative crowd reviews and prioritizing feedback to be satisfied in order to increase the apps success.},
	eventtitle = {2015 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {291--300},
	booktitle = {2015 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	author = {Palomba, Fabio and Linares-Vásquez, Mario and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and Poshyvanyk, Denys and De Lucia, Andrea},
	date = {2015-09},
	year = {2015},
	keywords = {Android apps, Androids, app stores, {CRISTAL}, crowd request, crowdsourced review tracking, Entropy, Feature extraction, Humanoid robots, informative crowd review tracing, Joining processes, mobile apps, mobile computing, Monitoring, Planning, ratings, recommender systems, source code (software), source code change, specialized recommendation systems, user reaction, user review},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/FF52VFVD/7332475.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/YXLJT7S2/Palomba et al. - 2015 - User reviews matter! Tracking crowdsourced reviews.pdf:application/pdf}
}

@book{pohl_requirements_2015,
	location = {Santa Barbara, {CA}},
	edition = {2.},
	title = {Requirements engineering fundamentals: a study guide for the certified professional for requirements engineering exam, foundation level, {IREB} compliant},
	isbn = {978-1-937538-77-4},
	shorttitle = {Requirements engineering fundamentals},
	pagetotal = {163},
	publisher = {Rocky Nook},
	author = {Pohl, Klaus and Rupp, Chris},
	date = {2015},
	year={2015},
	langid = {english},
	keywords = {Electronic data processing documentation, Examinations, Requirements engineering, Software engineering, Study guides, System design},
	file = {Pohl and Rupp - 2015 - Requirements engineering fundamentals a study gui.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/TP2GFUT7/Pohl and Rupp - 2015 - Requirements engineering fundamentals a study gui.pdf:application/pdf}
}

@article{maalej_toward_2016,
	title = {Toward Data-Driven Requirements Engineering},
	volume = {33},
	issn = {1937-4194},
	doi = {10.1109/MS.2015.153},
	abstract = {Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.},
	pages = {48--54},
	number = {1},
	journaltitle = {{IEEE} Software},
	author = {Maalej, Walid and Nayebi, Maleknaz and Johann, Timo and Ruhe, Guenther},
	date = {2016-01},
	year={2016},
	note = {Conference Name: {IEEE} Software},
	keywords = {app reviews, data-driven requirements engineering, data-driven user-centered software requirement identification, data-driven user-centered software requirement management, data-driven user-centered software requirement prioritization, decision support, error logs, explicit user data, Feature extraction, formal specification, implicit user data, Market research, Media, requirements engineering, Requirements engineering, sensor data, software analytics, software development, software engineering, Software engineering, software management, software products, software vendors, Stakeholders, usage data, user feedback},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/66BYR224/7325177.html:text/html}
}

@inproceedings{maalej_data-driven_2019,
	title = {Data-Driven Requirements Engineering - An Update},
	doi = {10.1109/ICSE-SEIP.2019.00041},
	abstract = {Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. In this talk we will present and discuss most recent achievements in this direction since the paper's original publication. We will also show to mine data sets mobile apps, give a few success/failure stories and a few practical advises.},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	pages = {289--290},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	author = {Maalej, Walid and Nayebi, Maleknaz and Ruhe, Guenther},
	date = {2019-05},
	year = {2019},
	keywords = {app stores, data mining, data set mining, data-driven requirements engineering, Internet, mobile apps, mobile computing, Requirements engineering, Data analytics, Mining software repositories, Stakeholders, Feature extraction, sensor data, social media, software engineering, software management, software products, software requirements management, software vendors, systems analysis},
	file = {IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/5C9EU64J/Maalej et al. - 2019 - Data-Driven Requirements Engineering - An Update.pdf:application/pdf}
}

@inproceedings{wu_topic_2017,
	title = {Topic mover's distance based document classification},
	doi = {10.1109/ICCT.2017.8359979},
	abstract = {We propose the Topic Mover's Distance ({TMD}), a new topic-based distance metric for documents, which is inspired from recently proposed Word Mover's Distance ({WMD}). Similar to {WMD}, {TMD} metric measures the similarity between two documents as the minimum amount of distance that the topics in one document need to travel to the topics in the other document. In our scheme, topics are the basic units to modeling documents, which are clustered from a general word-word co-occurrence matrix by Poisson Infinite Relational Model ({PIRM}) and vectorized by Glove embedding algorithm. Experiments for document classification on six real world datasets show that compared with word-based {WMD}, the proposed {TMD} can achieve much lower time complexity with the same accuracy.},
	eventtitle = {2017 {IEEE} 17th International Conference on Communication Technology ({ICCT})},
	pages = {1998--2002},
	booktitle = {2017 {IEEE} 17th International Conference on Communication Technology ({ICCT})},
	author = {Wu, Xinhui and Li, Hui},
	date = {2017-10},
	year = {2017},
	note = {{ISSN}: 2576-7828},
	keywords = {Classification algorithms, Clustering algorithms, computational complexity, document classification, document handling, Glove embedding algorithm, {IRM}, low time complexity, Measurement, pattern classification, Poisson infinite relational model, Semantics, time complexity, Time complexity, {TMD} metric, topic, topic movers distance, topic-based distance metric, Vocabulary, {WMD}, word-word co-occurrence matrix},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/YKAUTNAC/8359979.html:text/html}
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	date = {2003-01},
	year = {2003}
}

@article{cavnar_n-gram-based_nodate,
	title = {N-Gram-Based Text Categorization},
	abstract = {Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difﬁculty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through {OCR}. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems.},
	pages = {14},
	author = {Cavnar, William B and Trenkle, John M},
	langid = {english},
	date = {2001-05},
	year = {2001},
	file = {Cavnar and Trenkle - N-Gram-Based Text Categorization.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/ECPE6WR2/Cavnar and Trenkle - N-Gram-Based Text Categorization.pdf:application/pdf}
}

@inproceedings{chu_data_2016,
	location = {San Francisco, California, {USA}},
	title = {Data Cleaning: Overview and Emerging Challenges},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2912574},
	doi = {10.1145/2882903.2912574},
	shorttitle = {Data Cleaning},
	abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the ﬁeld, we will ﬁrst present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-theart techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efﬁciency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
	eventtitle = {the 2016 International Conference},
	pages = {2201--2206},
	booktitle = {Proceedings of the 2016 International Conference on Management of Data - {SIGMOD} '16},
	publisher = {{ACM} Press},
	author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
	urldate = {2020-02-23},
	date = {2016},
	year = {2016},
	langid = {english},
	file = {Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/3MDNSLX3/Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:application/pdf}
}

@inproceedings{ferrari_natural_2018,
	location = {Gothenburg, Sweden},
	title = {Natural language requirements processing: from research to practice},
	isbn = {978-1-4503-5663-3},
	url = {http://dl.acm.org/citation.cfm?doid=3183440.3183467},
	doi = {10.1145/3183440.3183467},
	shorttitle = {Natural language requirements processing},
	abstract = {Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering ({RE}) researchers for more than two decades. Recent technological advancements in natural language processing ({NLP}) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what {NLP} can do today for {RE} problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of {NLP} to {RE} problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in {NLP} that can play a role in {RE} research, including distributional semantics representations; discuss criteria for evaluation of {NLP} techniques in the {RE} context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.},
	eventtitle = {the 40th International Conference},
	pages = {536--537},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering Companion Proceeedings},
	publisher = {{ACM} Press},
	author = {Ferrari, Alessio},
	urldate = {2019-11-15},
	year = {2018},
	langid = {english},
	file = {Ferrari - 2018 - Natural language requirements processing from res.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/6GMZTLWA/Ferrari - 2018 - Natural language requirements processing from res.pdf:application/pdf}
}

@article{francis_standard_1965,
	title = {A Standard Corpus of Edited Present-Day American English},
	volume = {26},
	issn = {0010-0994},
	url = {https://www.jstor.org/stable/373638},
	doi = {10.2307/373638},
	pages = {267--273},
	number = {4},
	journaltitle = {College English},
	author = {Francis, W. Nelson},
	urldate = {2020-01-15},
	year = {1965}
}


@unpublished{krishnan_data_2016,
	title = {Data Cleaning: A Statistical Perspective -  Overview and Challenges Part 2},
	url = {https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxkYXRhY2xlYW5pbmd0dXRvcmlhbHNpZ21vZDE2fGd4OjJhMzc4ZWExM2U3MzA3MGE},
	note = {{ACM} {SIGMOD}/{PODS} Conference},
	author = {Krishnan, Sanjay and Wang, Jiannan},
	urldate = {2020-02-23},
	date = {2016-06},
	year = {2016},
	file = {Part2.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/75P2ZJUC/viewer.html:text/html}
}

@inproceedings{kusner_word_2015,
	location = {Lille, France},
	title = {From word embeddings to document distances},
	series = {{ICML}'15},
	abstract = {We present the Word Mover's Distance ({WMD}), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The {WMD} distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the {WMD} metric leads to unprecedented low k-nearest neighbor document classification error rates.},
	pages = {957--966},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	publisher = {{JMLR}.org},
	author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
	urldate = {2020-01-18},
	date = {2015-07-06},
	year = {2015},
	file = {Kusner et al. - From Word Embeddings To Document Distances.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/G42GVQCV/Kusner et al. - From Word Embeddings To Document Distances.pdf:application/pdf}
}

@inproceedings{li_classifying_2019,
	location = {San Francisco, {CA}, {USA}},
	title = {Classifying Extremely Short Texts by Exploiting Semantic Centroids in Word Mover's Distance Space},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313397},
	doi = {10.1145/3308558.3313397},
	series = {{WWW} '19},
	abstract = {Automatically classifying extremely short texts, such as social media posts and web page titles, plays an important role in a wide range of content analysis applications. However, traditional classifiers based on bag-of-words ({BoW}) representations often fail in this task. The underlying reason is that the document similarity can not be accurately measured under {BoW} representations due to the extreme sparseness of short texts. This results in significant difficulty to capture the generality of short texts. To address this problem, we use a better regularized word mover's distance ({RWMD}), which can measure distances among short texts at the semantic level. We then propose a {RWMD}-based centroid classifier for short texts, named {RWMD}-{CC}. Basically, {RWMD}-{CC} computes a representative semantic centroid for each category under the {RWMD} measure, and predicts test documents by finding the closest semantic centroid. The testing is much more efficient than the prior art of K nearest neighbor classifier based on {WMD}. Experimental results indicate that our {RWMD}-{CC} can achieve very competitive classification performance on extremely short texts.},
	pages = {939--949},
	booktitle = {The World Wide Web Conference},
	publisher = {Association for Computing Machinery},
	author = {Li, Changchun and Ouyang, Jihong and Li, Ximing},
	urldate = {2020-02-22},
	date = {2019-05-13},
	year = {2019},
	keywords = {Extremely Short Texts, Hypothesis Margin, Regularized Word Mover's Distance, Semantic Centroid}
}

@article{maaten_visualizing_2008,
	title = {Visualizing Data using t-{SNE}},
	volume = {9},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	pages = {2579--2605},
	issue = {Nov},
	journaltitle = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	urldate = {2020-01-19},
	date = {2008},
	year = {2008},
	file = {Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/YX4NQT8H/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf;Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/EYG5EX6F/vandermaaten08a.html:text/html}
}

@inproceedings{mikolov_linguistic_2013,
	location = {Atlanta, Georgia},
	title = {Linguistic Regularities in Continuous Space Word Representations},
	url = {https://www.aclweb.org/anthology/N13-1090},
	eventtitle = {{NAACL}-{HLT} 2013},
	pages = {746--751},
	booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	urldate = {2020-02-23},
	date = {2013-06},
	year = {2013},
	file = {Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/AJK4GDFD/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf}
}

@article{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	volume = {26},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	journaltitle = {Advances in Neural Information Processing Systems},
	shortjournal = {Advances in Neural Information Processing Systems},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, G.s and Dean, Jeffrey},
	date = {2013-10-16},
	year = {2013},
	file = {Full Text PDF:/Users/kim/Zotero/storage/K69INQWX/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@inproceedings{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	pages = {1--12},
	author = {Mikolov, Tomas and Corrado, G.s and Chen, Kai and Dean, Jeffrey},
	date = {2013-01-01},
	year = {2013},
	file = {arXiv Fulltext PDF:/Users/kim/Zotero/storage/A4NB64A4/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf}
}

@inproceedings{murukannaiah_toward_2017,
	location = {Lisbon, Portugal},
	title = {Toward Automating Crowd {RE}},
	isbn = {978-1-5386-3191-1},
	url = {http://ieeexplore.ieee.org/document/8049175/},
	doi = {10.1109/RE.2017.74},
	abstract = {Crowd {RE} is an emerging avenue for engaging the general public or the so called crowd in variety of requirements engineering tasks. Crowd {RE} scales {RE} by involving, potentially, millions of users. Although humans are at the center of Crowd {RE}, automated techniques are necessary (1) to derive useful insights from large amounts of raw data the crowd can produce; and (2) to drive the Crowd {RE} process, itself, by facilitating novel workﬂows combining crowd and machine intelligence.},
	eventtitle = {2017 {IEEE} 25th International Requirements Engineering Conference ({RE})},
	year = {2017},
	pages = {512--515},
	booktitle = {2017 {IEEE} 25th International Requirements Engineering Conference ({RE})},
	publisher = {{IEEE}},
	author = {Murukannaiah, Pradeep K. and Ajmeri, Nirav and Singh, Munindar P.},
	urldate = {2019-11-04},
	date = {2017-09},
	langid = {english},
	file = {Murukannaiah et al. - 2017 - Toward Automating Crowd RE.pdf:/Users/kim/Zotero/storage/35GVELY5/Murukannaiah et al. - 2017 - Toward Automating Crowd RE.pdf:application/pdf}
}

@article{quan_short_2015,
	title = {Short and Sparse Text Topic Modeling via Self-Aggregation},
	abstract = {The overwhelming amount of short text data on social media and elsewhere has posed great challenges to topic modeling due to the sparsity problem. Most existing attempts to alleviate this problem resort to heuristic strategies to aggregate short texts into pseudo-documents before the application of standard topic modeling. Although such strategies cannot be well generalized to more general genres of short texts, the success has shed light on how to develop a generalized solution. In this paper, we present a novel model towards this goal by integrating topic modeling with short text aggregation during topic inference. The aggregation is founded on general topical afﬁnity of texts rather than particular heuristics, making the model readily applicable to various short texts. Experimental results on real-world datasets validate the effectiveness of this new model, suggesting that it can distill more meaningful topics from short texts.},
	pages = {2270--2276},
	journaltitle = {Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence},
	author = {Quan, Xiaojun and Kit, Chunyu and Ge, Yong and Pan, Sinno Jialin},
	date = {2015},
	year = {2015},
	langid = {english},
	file = {Quan et al. - Short and Sparse Text Topic Modeling via Self-Aggr.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/NKGRVC5H/Quan et al. - Short and Sparse Text Topic Modeling via Self-Aggr.pdf:application/pdf}
}


@article{niu_topic2vec_2015,
	title = {Topic2Vec: Learning Distributed Representations of Topics},
	url = {http://arxiv.org/abs/1506.08422},
	shorttitle = {Topic2Vec},
	abstract = {Latent Dirichlet Allocation ({LDA}) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from {LDA} only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than {LDA}-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.},
	journaltitle = {{arXiv}:1506.08422 [cs]},
	author = {Niu, Li-Qiang and Dai, Xin-Yu},
	urldate = {2020-02-23},
	date = {2015-06-28},
	year = {2015},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.08422},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Niu and Dai - 2015 - Topic2Vec Learning Distributed Representations of.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/PEYSB3CE/Niu and Dai - 2015 - Topic2Vec Learning Distributed Representations of.pdf:application/pdf}
}

@article{qiang_topic_2016,
	title = {Topic Modeling over Short Texts by Incorporating Word Embeddings},
	url = {http://arxiv.org/abs/1609.08496},
	abstract = {Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest proﬁling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis ({PLSA}) and latent Dirichlet allocation ({LDA}) cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn semantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model ({ETM}), to learn latent topics from short texts. {ETM} not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudotexts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the eﬀectiveness of our model comparing with the state-of-the-art models.},
	journaltitle = {{arXiv}:1609.08496 [cs]},
	author = {Qiang, Jipeng and Chen, Ping and Wang, Tong and Wu, Xindong},
	urldate = {2020-02-24},
	date = {2016-09-27},
	year = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.08496},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {Qiang et al. - 2016 - Topic Modeling over Short Texts by Incorporating W.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/8GB2ZEK5/Qiang et al. - 2016 - Topic Modeling over Short Texts by Incorporating W.pdf:application/pdf}
}

@inproceedings{rehurek_software_2010,
	location = {Valletta, Malta},
	title = {Software Framework for Topic Modelling with Large Corpora},
	isbn = {978-2-9517408-6-0},
	url = {http://is.muni.cz/publication/884893/en},
	abstract = {Large corpora are ubiquitous in today's world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model ({VSM}). We identify gap in existing {VSM} implementations, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. In this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library {DML}-{CZ}.},
	eventtitle = {{LREC} 2010 workshop},
	booktitle = {New Challenges for {NLP} Frameworks},
	publisher = {{ELRA}},
	author = {Řehůřek, Radim and Sojka, Petr},
	urldate = {2020-01-19},
	date = {2010-05-22},
	year = {2010},
	file = {lrec2010_final.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/9RH9HSI3/lrec2010_final.pdf:application/pdf;Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/YB8DHW6S/en.html:text/html}
}

@inproceedings{solangi_review_2018,
	title = {Review on Natural Language Processing ({NLP}) and Its Toolkits for Opinion Mining and Sentiment Analysis},
	doi = {10.1109/ICETAS.2018.8629198},
	abstract = {As the majority of online networking on the Internet, opinion mining has turned into a fundamental way to deal with investigating such huge numbers of information. Different applications show up in an extensive variety of modern areas. In the interim, opinions have various pronunciations which bring along investigate challenges. The research challenges make opinion mining a dynamic research region recently. In this paper, Natural Language Processing ({NLP}) techniques for opinion mining and sentiment analysis are reviewed. Initially {NLP} is reviewed then briefed about its common and useful preprocessing steps also. In this paper opinion mining for various levels are analyzed and reviewed. At the end issues are identified and some recommendation are suggested for opinion mining and-sentiment-analysis.},
	eventtitle = {2018 {IEEE} 5th International Conference on Engineering Technologies and Applied Sciences ({ICETAS})},
	pages = {1--4},
	booktitle = {2018 {IEEE} 5th International Conference on Engineering Technologies and Applied Sciences ({ICETAS})},
	author = {Solangi, Yasir Ali and Solangi, Zulfiqar Ali and Aarain, Samreen and Abro, Amna and Mallah, Ghulam Ali and Shah, Asadullah},
	date = {2018-11},
	year = {2018},
	keywords = {data mining, Data mining, interim opinions, Internet, Labeling, natural language processing, natural language processing techniques, {NLP}, online networking, opinion mining, Opinion mining, Semantics, sentiment analysis, Sentiment analysis, Tokenization, toolkits, Tools},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/E3G6JH7S/8629198.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/8GJKALER/Solangi et al. - 2018 - Review on Natural Language Processing (NLP) and It.pdf:application/pdf}
}

@article{suen_n-gram_1979,
	title = {n-Gram Statistics for Natural Language Understanding and Text Processing},
	volume = {{PAMI}-1},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.1979.4766902},
	abstract = {n-gram (n = 1 to 5) statistics and other properties of the English language were derived for applications in natural language understanding and text processing. They were computed from a well-known corpus composed of 1 million word samples. Similar properties were also derived from the most frequent 1000 words of three other corpuses. The positional distributions of n-grams obtained in the present study are discussed. Statistical studies on word length and trends of n-gram frequencies versus vocabulary are presented. In addition to a survey of n-gram statistics found in the literature, a collection of n-gram statistics obtained by other researchers is reviewed and compared.},
	pages = {164--172},
	number = {2},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Suen, Ching Y.},
	date = {1979-04},
	year = {1979},
	keywords = {Application software, Character recognition, context, Error correction, Frequency, Humans, language understanding, n-gram statistics, Natural languages, Optical character recognition software, positional distributions of letters, Statistical distributions, Statistics, text processing, Text processing, Vocabulary, word length analysis},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/KJN8CKJG/4766902.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/6BLWMPF3/Suen - 1979 - n-Gram Statistics for Natural Language Understandi.pdf:application/pdf}
}

@inproceedings{gemkow_automatic_2018,
	address = {Banff, AB},
	title = {Automatic {Glossary} {Term} {Extraction} from {Large}-{Scale} {Requirements} {Specifications}},
	isbn = {978-1-5386-7418-5},
	url = {https://ieeexplore.ieee.org/document/8491159/},
	doi = {10.1109/RE.2018.00052},
	abstract = {Creating glossaries for large corpora of requirments is an important but expensive task. Glossary term extraction methods often focus on achieving a high recall rate and, therefore, favor linguistic proecssing for extracting glossary term candidates and neglect the beneﬁts from reducing the number of candidates by statistical ﬁlter methods. However, especially for large datasets a reduction of the likewise large number of candidates may be crucial. This paper demonstrates how to automatically extract relevant domain-speciﬁc glossary term candidates from a large body of requirements, the CrowdRE dataset. Our hybrid approach combines linguistic processing and statistical ﬁltering for extracting and reducing glossary term candidates. In a twofold evaluation, we examine the impact of our approach on the quality and quantity of extracted terms. We provide a ground truth for a subset of the requirements and show that a substantial degree of recall can be achieved. Furthermore, we advocate requirements coverage as an additional quality metric to assess the term reduction that results from our statistical ﬁlters. Results indicate that with a careful combination of linguistic and statistical extraction methods, a fair balance between later manual efforts and a high recall rate can be achieved.},
	language = {en},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Gemkow, Tim and Conzelmann, Miro and Hartig, Kerstin and Vogelsang, Andreas},
	month = aug,
	year = {2018},
	pages = {412--417}
}

@inproceedings{george_unsupervised_2018,
	title = {Unsupervised {Topic} {Detection} based on 2D {Vector} {Space} model using {Apriori} {Algorithm} and {NLP}},
	doi = {10.1109/ICDIM.2018.8846982},
	abstract = {Topic modelling is an approach in data mining, use machine learning methods to discover patterns in large amount of unstructured text. It takes a collection of documents and group the words into clusters of words that we call Bag of words, and identify topics by using process of similarity. Topic modelling provides us with methods to organize, understand and summarize large collections of textual information. There are a lot of approaches have been exposed for Topic modelling, the most in use are Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) and explicit semantic analysis (ESA). In our study we describing an approach to refine Topic detection based on 2d vector space model VSM by using Apriori algorithm along with Natural language processing, to form a better connected terms in vector space for clean engagement with the query.},
	booktitle = {2018 {Thirteenth} {International} {Conference} on {Digital} {Information} {Management} ({ICDIM})},
	author = {George, Michael},
	month = sep,
	year = {2018},
	note = {ISSN: null},
	keywords = {Apriori, Apriori algorithm, Clustering algorithms, Data models, Itemsets, NLP, Natural language processing, Semantics, Two dimensional displays, Unsupervised Topic Models, Vector space model, bag of words, data mining, document handling, explicit semantic analysis, information retrieval, latent dirichlet allocation, latent semantic analysis, learning (artificial intelligence), machine learning methods, natural language processing, pattern clustering, text analysis, topic modelling, unsupervised topic detection, vector space},
	pages = {279--283}
}

@article{wold_principal_1987,
	title = {Principal Component Analysis},
	volume = {2},
	pages = {37--52},
	journaltitle = {Tutorial n Chemometrics and Intelligent Laboratory Systems,},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
	year = {1987},
	langid = {english},
	file = {Wold et al. - Principal Component Analysis.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/NJVYEUIL/Wold et al. - Principal Component Analysis.pdf:application/pdf}
}

@article{zhou_tong_text_2016,
	title = {A {TEXT} {MINING} {RESEARCH} {BASED} {ON}
LDA {TOPIC} {MODELLING}},
	doi = {10.5121},
	abstract = {A Large number of digital text information is generated every day. Effectively searching,
managing and exploring the text data has become a main task. In this paper, we first represent
an introduction to text mining and a probabilistic topic model Latent Dirichlet allocation. Then
two experiments are proposed - Wikipedia articles and users’ tweets topic modelling. The
former one builds up a document topic model, aiming to a topic perspective solution on
searching, exploring and recommending articles. The latter one sets up a user topic model,
providing a full research and analysis over Twitter users’ interest. The experiment process
including data collecting, data pre-processing and model training is fully documented and
commented. Further more, the conclusion and application of this paper could be a useful
computation tool for social and business research.},
	journal = {Computer Science \& Information Technology (CS \& IT)},
	author = {Zhou Tong, Haiyi Zhang},
	year = {2016},
	pages = {201--210}
}

@online{gensim_python,
	title = {Gensim {Python} library},
	url = {https://radimrehurek.com/gensim/index.html},
	note = {last visited: 2020-01-19}
}

@online{nltk_library,
	title = {NLTK Library},
	url = {https://www.nltk.org/},
	note = {last visited: 2020-01-18}
}

@online{crowdre_dataset,
	title = {Crowd {RE} {Dataset}},
	url = {https://crowdre.github.io/murukannaiah-smarthome-requirements-dataset/},
	note = {last visited: 2020-01-15}
}

@online{google_embeddings_figure,
	title = {{Machine} {Learning} {Crash} {Course} {\textbar} {Embeddings:} {Translating} to a {Lower-Dimensional} {Space}},
	url = {https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space},
	shorttitle = {Embeddings},
	titleaddon = {Google Developers},
	note = {last visited: 2020-02-24},
	urldate = {2020-02-24},
	langid = {english},
	file = {Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/9GCZE69F/translating-to-a-lower-dimensional-space.html:text/html}
}

@online{tensorflow_word_embeddings,
	title = {Word embeddings {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/tutorials/text/word_embeddings},
	titleaddon = {{TensorFlow}},
	note = {last visited: 2020-02-24},
	urldate = {2020-02-24},
	langid = {english},
	file = {Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/G85W85DS/word_embeddings.html:text/html}
}

@inproceedings{mhatre_dimensionality_2017,
	title = {Dimensionality {Reduction} for {Sentiment} {Analysis} using {Pre}-processing {Techniques}},
	isbn = {978-1-5090-4890-8},
	abstract = {Sentiment analysis is the study of people’s opinions, sentiments, attitudes and emotions, expressed in written language but this process is time consuming, inconsistent and costly in business context. Pre-processing the data will help to ease this difficulty. Pre-processing is the process of cleaning and preparing the text for its analysis using pre-processing techniques. The existing pre-processing techniques are Handling Expressive Lengthening, Emoticons Handling, HTML Tags Removal, Punctuations Handling, Slangs Handling, Stopwords Removal, Stemming and Lemmatization. In this paper, the effect of various pre-processing techniques and their combinations was analyzed on the dataset taken from Kaggle called Bag of Words Meets Bags of Popcorn. By taking every possible combination of pre-processing techniques, the aim was to find the one giving highest accuracy. Random Forest Classifier was used to predict sentiments as it is known to give good accuracy and the result was evaluated using 10 fold cross validation method. Accuracy increased from unprocessed data to pre-processed data. It was concluded that using pre-processing techniques gives a higher accuracy than the traditional approach i.e. no pre-processing.},
	booktitle = {Proceedings of the {IEEE} 2017 {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	author = {Mhatre, Mayuri and Phondekar, Dakshata and Kadam, Pranali and Chawathe, Anushka and Ghag, Kranti},
	year = {2017},
	pages = {16--21}
}

@incollection{leskovec_data_2014,
	title = {Data {Mining}},
	booktitle = {Mining of {Massive} {Datasets}},
	author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
	year = {2014}
}
