% Encoding: UTF-8
@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30}
}

@article{cavnar_n-gram-based_nodate,
	title = {N-Gram-Based Text Categorization},
	abstract = {Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difﬁculty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through {OCR}. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems.},
	pages = {14},
	author = {Cavnar, William B and Trenkle, John M},
	langid = {english},
	file = {Cavnar and Trenkle - N-Gram-Based Text Categorization.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/ECPE6WR2/Cavnar and Trenkle - N-Gram-Based Text Categorization.pdf:application/pdf}
}

@inproceedings{chu_data_2016,
	location = {San Francisco, California, {USA}},
	title = {Data Cleaning: Overview and Emerging Challenges},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2912574},
	doi = {10.1145/2882903.2912574},
	shorttitle = {Data Cleaning},
	abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the ﬁeld, we will ﬁrst present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-theart techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efﬁciency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
	eventtitle = {the 2016 International Conference},
	pages = {2201--2206},
	booktitle = {Proceedings of the 2016 International Conference on Management of Data - {SIGMOD} '16},
	publisher = {{ACM} Press},
	author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
	urldate = {2020-02-23},
	date = {2016},
	langid = {english},
	file = {Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/3MDNSLX3/Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:application/pdf}
}

@inproceedings{ferrari_natural_2018,
	location = {Gothenburg, Sweden},
	title = {Natural language requirements processing: from research to practice},
	isbn = {978-1-4503-5663-3},
	url = {http://dl.acm.org/citation.cfm?doid=3183440.3183467},
	doi = {10.1145/3183440.3183467},
	shorttitle = {Natural language requirements processing},
	abstract = {Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering ({RE}) researchers for more than two decades. Recent technological advancements in natural language processing ({NLP}) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what {NLP} can do today for {RE} problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of {NLP} to {RE} problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in {NLP} that can play a role in {RE} research, including distributional semantics representations; discuss criteria for evaluation of {NLP} techniques in the {RE} context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.},
	eventtitle = {the 40th International Conference},
	pages = {536--537},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering Companion Proceeedings - {ICSE} '18},
	publisher = {{ACM} Press},
	author = {Ferrari, Alessio},
	urldate = {2019-11-15},
	date = {2018},
	langid = {english},
	file = {Ferrari - 2018 - Natural language requirements processing from res.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/6GMZTLWA/Ferrari - 2018 - Natural language requirements processing from res.pdf:application/pdf}
}

@article{francis_standard_1965,
	title = {A Standard Corpus of Edited Present-Day American English},
	volume = {26},
	issn = {0010-0994},
	url = {https://www.jstor.org/stable/373638},
	doi = {10.2307/373638},
	pages = {267--273},
	number = {4},
	journaltitle = {College English},
	author = {Francis, W. Nelson},
	urldate = {2020-01-15},
	date = {1965}
}


@unpublished{krishnan_data_2016,
	title = {Data Cleaning: A Statistical Perspective -  Overview and Challenges Part 2},
	url = {https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxkYXRhY2xlYW5pbmd0dXRvcmlhbHNpZ21vZDE2fGd4OjJhMzc4ZWExM2U3MzA3MGE},
	note = {{ACM} {SIGMOD}/{PODS} Conference},
	author = {Krishnan, Sanjay and Wang, Jiannan},
	urldate = {2020-02-23},
	date = {2016-06},
	file = {Part2.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/75P2ZJUC/viewer.html:text/html}
}

@inproceedings{kusner_word_2015,
	location = {Lille, France},
	title = {From word embeddings to document distances},
	series = {{ICML}'15},
	abstract = {We present the Word Mover's Distance ({WMD}), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The {WMD} distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the {WMD} metric leads to unprecedented low k-nearest neighbor document classification error rates.},
	pages = {957--966},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	publisher = {{JMLR}.org},
	author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
	urldate = {2020-01-18},
	date = {2015-07-06},
	file = {Kusner et al. - From Word Embeddings To Document Distances.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/G42GVQCV/Kusner et al. - From Word Embeddings To Document Distances.pdf:application/pdf}
}

@inproceedings{li_classifying_2019,
	location = {San Francisco, {CA}, {USA}},
	title = {Classifying Extremely Short Texts by Exploiting Semantic Centroids in Word Mover's Distance Space},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313397},
	doi = {10.1145/3308558.3313397},
	series = {{WWW} '19},
	abstract = {Automatically classifying extremely short texts, such as social media posts and web page titles, plays an important role in a wide range of content analysis applications. However, traditional classifiers based on bag-of-words ({BoW}) representations often fail in this task. The underlying reason is that the document similarity can not be accurately measured under {BoW} representations due to the extreme sparseness of short texts. This results in significant difficulty to capture the generality of short texts. To address this problem, we use a better regularized word mover's distance ({RWMD}), which can measure distances among short texts at the semantic level. We then propose a {RWMD}-based centroid classifier for short texts, named {RWMD}-{CC}. Basically, {RWMD}-{CC} computes a representative semantic centroid for each category under the {RWMD} measure, and predicts test documents by finding the closest semantic centroid. The testing is much more efficient than the prior art of K nearest neighbor classifier based on {WMD}. Experimental results indicate that our {RWMD}-{CC} can achieve very competitive classification performance on extremely short texts.},
	pages = {939--949},
	booktitle = {The World Wide Web Conference},
	publisher = {Association for Computing Machinery},
	author = {Li, Changchun and Ouyang, Jihong and Li, Ximing},
	urldate = {2020-02-22},
	date = {2019-05-13},
	keywords = {Extremely Short Texts, Hypothesis Margin, Regularized Word Mover's Distance, Semantic Centroid}
}

@article{maaten_visualizing_2008,
	title = {Visualizing Data using t-{SNE}},
	volume = {9},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	pages = {2579--2605},
	issue = {Nov},
	journaltitle = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	urldate = {2020-01-19},
	date = {2008},
	file = {Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/YX4NQT8H/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf;Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/EYG5EX6F/vandermaaten08a.html:text/html}
}

@inproceedings{mikolov_linguistic_2013,
	location = {Atlanta, Georgia},
	title = {Linguistic Regularities in Continuous Space Word Representations},
	url = {https://www.aclweb.org/anthology/N13-1090},
	eventtitle = {{NAACL}-{HLT} 2013},
	pages = {746--751},
	booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	urldate = {2020-02-23},
	date = {2013-06},
	file = {Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/AJK4GDFD/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf}
}

@article{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	volume = {26},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	journaltitle = {Advances in Neural Information Processing Systems},
	shortjournal = {Advances in Neural Information Processing Systems},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, G.s and Dean, Jeffrey},
	date = {2013-10-16},
	file = {Full Text PDF:/Users/kim/Zotero/storage/K69INQWX/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@inproceedings{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	pages = {1--12},
	author = {Mikolov, Tomas and Corrado, G.s and Chen, Kai and Dean, Jeffrey},
	date = {2013-01-01},
	file = {arXiv Fulltext PDF:/Users/kim/Zotero/storage/A4NB64A4/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf}
}

@inproceedings{murukannaiah_toward_2017,
	location = {Lisbon, Portugal},
	title = {Toward Automating Crowd {RE}},
	isbn = {978-1-5386-3191-1},
	url = {http://ieeexplore.ieee.org/document/8049175/},
	doi = {10.1109/RE.2017.74},
	abstract = {Crowd {RE} is an emerging avenue for engaging the general public or the so called crowd in variety of requirements engineering tasks. Crowd {RE} scales {RE} by involving, potentially, millions of users. Although humans are at the center of Crowd {RE}, automated techniques are necessary (1) to derive useful insights from large amounts of raw data the crowd can produce; and (2) to drive the Crowd {RE} process, itself, by facilitating novel workﬂows combining crowd and machine intelligence.},
	eventtitle = {2017 {IEEE} 25th International Requirements Engineering Conference ({RE})},
	pages = {512--515},
	booktitle = {2017 {IEEE} 25th International Requirements Engineering Conference ({RE})},
	publisher = {{IEEE}},
	author = {Murukannaiah, Pradeep K. and Ajmeri, Nirav and Singh, Munindar P.},
	urldate = {2019-11-04},
	date = {2017-09},
	langid = {english},
	file = {Murukannaiah et al. - 2017 - Toward Automating Crowd RE.pdf:/Users/kim/Zotero/storage/35GVELY5/Murukannaiah et al. - 2017 - Toward Automating Crowd RE.pdf:application/pdf}
}


@article{niu_topic2vec_2015,
	title = {Topic2Vec: Learning Distributed Representations of Topics},
	url = {http://arxiv.org/abs/1506.08422},
	shorttitle = {Topic2Vec},
	abstract = {Latent Dirichlet Allocation ({LDA}) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from {LDA} only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than {LDA}-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.},
	journaltitle = {{arXiv}:1506.08422 [cs]},
	author = {Niu, Li-Qiang and Dai, Xin-Yu},
	urldate = {2020-02-23},
	date = {2015-06-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.08422},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Niu and Dai - 2015 - Topic2Vec Learning Distributed Representations of.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/PEYSB3CE/Niu and Dai - 2015 - Topic2Vec Learning Distributed Representations of.pdf:application/pdf}
}

@inproceedings{rehurek_software_2010,
	location = {Valletta, Malta},
	title = {Software Framework for Topic Modelling with Large Corpora},
	isbn = {978-2-9517408-6-0},
	url = {http://is.muni.cz/publication/884893/en},
	abstract = {Large corpora are ubiquitous in today's world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model ({VSM}). We identify gap in existing {VSM} implementations, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. In this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library {DML}-{CZ}.},
	eventtitle = {{LREC} 2010 workshop},
	booktitle = {New Challenges for {NLP} Frameworks},
	publisher = {{ELRA}},
	author = {Řehůřek, Radim and Sojka, Petr},
	urldate = {2020-01-19},
	date = {2010-05-22},
	file = {lrec2010_final.pdf:/Volumes/Home/Users/kim/Applications/Zotero/storage/9RH9HSI3/lrec2010_final.pdf:application/pdf;Snapshot:/Volumes/Home/Users/kim/Applications/Zotero/storage/YB8DHW6S/en.html:text/html}
}

@inproceedings{solangi_review_2018,
	title = {Review on Natural Language Processing ({NLP}) and Its Toolkits for Opinion Mining and Sentiment Analysis},
	doi = {10.1109/ICETAS.2018.8629198},
	abstract = {As the majority of online networking on the Internet, opinion mining has turned into a fundamental way to deal with investigating such huge numbers of information. Different applications show up in an extensive variety of modern areas. In the interim, opinions have various pronunciations which bring along investigate challenges. The research challenges make opinion mining a dynamic research region recently. In this paper, Natural Language Processing ({NLP}) techniques for opinion mining and sentiment analysis are reviewed. Initially {NLP} is reviewed then briefed about its common and useful preprocessing steps also. In this paper opinion mining for various levels are analyzed and reviewed. At the end issues are identified and some recommendation are suggested for opinion mining and-sentiment-analysis.},
	eventtitle = {2018 {IEEE} 5th International Conference on Engineering Technologies and Applied Sciences ({ICETAS})},
	pages = {1--4},
	booktitle = {2018 {IEEE} 5th International Conference on Engineering Technologies and Applied Sciences ({ICETAS})},
	author = {Solangi, Yasir Ali and Solangi, Zulfiqar Ali and Aarain, Samreen and Abro, Amna and Mallah, Ghulam Ali and Shah, Asadullah},
	date = {2018-11},
	note = {{ISSN}: null},
	keywords = {data mining, Data mining, interim opinions, Internet, Labeling, natural language processing, natural language processing techniques, {NLP}, online networking, opinion mining, Opinion mining, Semantics, sentiment analysis, Sentiment analysis, Tokenization, toolkits, Tools},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/E3G6JH7S/8629198.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/8GJKALER/Solangi et al. - 2018 - Review on Natural Language Processing (NLP) and It.pdf:application/pdf}
}

@article{suen_n-gram_1979,
	title = {n-Gram Statistics for Natural Language Understanding and Text Processing},
	volume = {{PAMI}-1},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.1979.4766902},
	abstract = {n-gram (n = 1 to 5) statistics and other properties of the English language were derived for applications in natural language understanding and text processing. They were computed from a well-known corpus composed of 1 million word samples. Similar properties were also derived from the most frequent 1000 words of three other corpuses. The positional distributions of n-grams obtained in the present study are discussed. Statistical studies on word length and trends of n-gram frequencies versus vocabulary are presented. In addition to a survey of n-gram statistics found in the literature, a collection of n-gram statistics obtained by other researchers is reviewed and compared.},
	pages = {164--172},
	number = {2},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Suen, Ching Y.},
	date = {1979-04},
	keywords = {Application software, Character recognition, context, Error correction, Frequency, Humans, language understanding, n-gram statistics, Natural languages, Optical character recognition software, positional distributions of letters, Statistical distributions, Statistics, text processing, Text processing, Vocabulary, word length analysis},
	file = {IEEE Xplore Abstract Record:/Volumes/Home/Users/kim/Applications/Zotero/storage/KJN8CKJG/4766902.html:text/html;IEEE Xplore Full Text PDF:/Volumes/Home/Users/kim/Applications/Zotero/storage/6BLWMPF3/Suen - 1979 - n-Gram Statistics for Natural Language Understandi.pdf:application/pdf}
}

@inproceedings{gemkow_automatic_2018,
	address = {Banff, AB},
	title = {Automatic {Glossary} {Term} {Extraction} from {Large}-{Scale} {Requirements} {Specifications}},
	isbn = {978-1-5386-7418-5},
	url = {https://ieeexplore.ieee.org/document/8491159/},
	doi = {10.1109/RE.2018.00052},
	abstract = {Creating glossaries for large corpora of requirments is an important but expensive task. Glossary term extraction methods often focus on achieving a high recall rate and, therefore, favor linguistic proecssing for extracting glossary term candidates and neglect the beneﬁts from reducing the number of candidates by statistical ﬁlter methods. However, especially for large datasets a reduction of the likewise large number of candidates may be crucial. This paper demonstrates how to automatically extract relevant domain-speciﬁc glossary term candidates from a large body of requirements, the CrowdRE dataset. Our hybrid approach combines linguistic processing and statistical ﬁltering for extracting and reducing glossary term candidates. In a twofold evaluation, we examine the impact of our approach on the quality and quantity of extracted terms. We provide a ground truth for a subset of the requirements and show that a substantial degree of recall can be achieved. Furthermore, we advocate requirements coverage as an additional quality metric to assess the term reduction that results from our statistical ﬁlters. Results indicate that with a careful combination of linguistic and statistical extraction methods, a fair balance between later manual efforts and a high recall rate can be achieved.},
	language = {en},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Gemkow, Tim and Conzelmann, Miro and Hartig, Kerstin and Vogelsang, Andreas},
	month = aug,
	year = {2018},
	pages = {412--417}
}

@inproceedings{george_unsupervised_2018,
	title = {Unsupervised {Topic} {Detection} based on 2D {Vector} {Space} model using {Apriori} {Algorithm} and {NLP}},
	doi = {10.1109/ICDIM.2018.8846982},
	abstract = {Topic modelling is an approach in data mining, use machine learning methods to discover patterns in large amount of unstructured text. It takes a collection of documents and group the words into clusters of words that we call Bag of words, and identify topics by using process of similarity. Topic modelling provides us with methods to organize, understand and summarize large collections of textual information. There are a lot of approaches have been exposed for Topic modelling, the most in use are Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) and explicit semantic analysis (ESA). In our study we describing an approach to refine Topic detection based on 2d vector space model VSM by using Apriori algorithm along with Natural language processing, to form a better connected terms in vector space for clean engagement with the query.},
	booktitle = {2018 {Thirteenth} {International} {Conference} on {Digital} {Information} {Management} ({ICDIM})},
	author = {George, Michael},
	month = sep,
	year = {2018},
	note = {ISSN: null},
	keywords = {Apriori, Apriori algorithm, Clustering algorithms, Data models, Itemsets, NLP, Natural language processing, Semantics, Two dimensional displays, Unsupervised Topic Models, Vector space model, bag of words, data mining, document handling, explicit semantic analysis, information retrieval, latent dirichlet allocation, latent semantic analysis, learning (artificial intelligence), machine learning methods, natural language processing, pattern clustering, text analysis, topic modelling, unsupervised topic detection, vector space},
	pages = {279--283}
}

@article{zhou_tong_text_2016,
	title = {A {TEXT} {MINING} {RESEARCH} {BASED} {ON}
LDA {TOPIC} {MODELLING}},
	doi = {10.5121},
	abstract = {A Large number of digital text information is generated every day. Effectively searching,
managing and exploring the text data has become a main task. In this paper, we first represent
an introduction to text mining and a probabilistic topic model Latent Dirichlet allocation. Then
two experiments are proposed - Wikipedia articles and users’ tweets topic modelling. The
former one builds up a document topic model, aiming to a topic perspective solution on
searching, exploring and recommending articles. The latter one sets up a user topic model,
providing a full research and analysis over Twitter users’ interest. The experiment process
including data collecting, data pre-processing and model training is fully documented and
commented. Further more, the conclusion and application of this paper could be a useful
computation tool for social and business research.},
	journal = {Computer Science \& Information Technology (CS \& IT)},
	author = {Zhou Tong, Haiyi Zhang},
	year = {2016},
	pages = {pp. 201--210}
}