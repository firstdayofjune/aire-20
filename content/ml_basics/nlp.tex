\subsection{Natural Language (Pre-)Processing} % (fold)
\label{sec:nlp}

In order to successfully perform an analysis of the dataset, we first needed to better understand the composition of the data. In a first step we therefore created and analyzed a corpus of requirements and compared the results to the Brown Corpus\cite{francis_standard_1965}, a much larger generic corpus with words taken from books and news articles.

\begin{table}
\centering
\begin{tabular}{ | l | c | c | } \hline
\thead{Indicator} & \thead{\crowdre{}} & \thead{Brown} \\ \hline
\makecell{Number of Tokens (unique)} & \makecell[c]{90,844 (5,024)} & 1,034,378 \\ \hline
Number of Lexical Words & 52,266 & 542,924 \\ \hline
\makecell{Vocabulary Size (Lexical Words)} & 4,906 & 4,6018 \\ \hline
Vocabulary Size (Stems) & 3,398 & 29,846 \\ \hline
\makecell{Average Sentence Length (Tokens)} & 31 & 18 \\ \hline
\makecell{Average Sentence Length (Lexical Words)} & 18 & 10 \\ \hline
Lexical Diversity & 0.011 & 0.054 \\ \hline
\end{tabular}
\caption{Data from the analysis of the \crowdre{} dataset}\label{tbl-dataset-analysis}
\end{table}


In \autoref{tbl-dataset-analysis} we can see the number of tokens and lexical words is much larger in the Brown dataset which is a result of a wider variety of words in this kind of texts and is also because the brown dataset contains approximately 10 times more lexical words than the \crowdre{} dataset.
Even though the requirement sentences tend to be much longer, which may have also been caused by the prescribed user story format, the lexical diversity is lower. Requirements use domain-specifc expressions, so the same or similar words appear more often in the written requirements\cite{ferrari_natural_2018}. And it is also necessary to use unique words for the description of the same feature to avoid ambiguity. To sum up we can say that the results are as expected from a dataset that contains only requirements.\\

In order to derive meaningful data from a dataset which is as small as ours, we had to perform some Natural Language Processing (NLP) first, before further analyzing the data. A range of NLP techniques exist, which can be used to prepare the data for our kind of analysis\cite{solangi_review_2018}\cite{ferrari_natural_2018}. The following list briefly describes the techniques we used in our research:

\begin{itemize}
	\item \textbf{Tokenization} means separating a the data into tokens. Where tokens which are basically words. With tokenization whitespaces and all punctuation is removed from the data. As result a list of tokens is generated. The easiest tokenization is just splitting all alphanumeric characters.
	\item \textbf{Stopword-Removal} is removing common words from the data. They are often only required because of grammar or syntax.
	\item \textbf{Stemming} is a technique that reduce a word to just the root of the word. It eliminates duplicates that have the same meaning. This important in NLP as the as the conjugation of a word is not important if we are just interested in the semantic information that is contained in the words.
	\item \textbf{Bag-of-Words} is a technique that is used to simplify a sentence or document. The idea is to have a list of all containing words with the corresponding word-count in the text. It therefore only holds the word itself and the multiplicity (or in other words the frequency).
	\item \textbf{TF-IDF} can be separated into two different indices. TF: term frequency is the number of is a rating how often a specific term occurs in the data. IDF: inverse document frequency is a measure how much information a single term provides in relation to a document. The TF-IDF therefore is a rating how valuable a term for a document is.
\end{itemize}