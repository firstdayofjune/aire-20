\section{Related Works} % (fold)
\label{sec:related_works}

Building up the preprocessing pipeline was done similar to another preprocessing pipeline that was used by \cite{gemkow_automatic_2018}. After the processing they did a different task but the results of the preprocessing was also an essential part for the glossary extraction.

Using an Latent Dirichlet Allocation for topic modelling is not a new idea. As supposed in \cite{zhou_tong_text_2016} they also used it for this task. They got good results with LDA but they also had a much bigger dataset as they gathered about 2000 articles from wikipedia and also users tweets from Twitter, that they used for their analyis.

As presented in \cite{george_unsupervised_2018} there are also several different approaches that can be used to perform the topic modelling task. They supposed a 2D vector space model. But again they had a much bigger dataset for their unsupervised approach and they also described that there is improvement required for the algorithms as the calculation consumes a lot of time.


\colorbox{yellow!30}{ToDo:} Add references to papers that have a similar approach...

- Paper about LDA for topic modelling