\section{Related Work} % (fold)
\label{sec:related_work}

Multiple recent works on topic modeling apply the Latent Dirichlet Allocation in order to get an appropriate result for the hidden topics. Zhou et. al in \cite{zhou_tong_text_2016} used this technique to automate a part of text mining. They used two different kind of dataset for their research. At first they focus on articles from Wikipedia where they evaluated over 200,000 articles. They found out that from 50 topics they discovered, there are three topics with high probabilities compared to the others. As second analysis they used a set of twitter messages from 10,000 users. They again found 30 topics containing five topics with the highest probabilities of the set of topics. As result they mentioned that the processing time of their approach took quite long and might be improved in future works.


Building up a pre-processing pipeline for the topic modeling approach was also performed at several related works. In \cite{gemkow_automatic_2018}  Gemko et. al proposed a data pre-processing pipeline they used for an automatic glossary term extraction. Their pipeline contains the steps of Tokenization, POS-Tagging, Chunking and Lemmatization. Additionally they apply some relevance filtering and specificity filtering afterwards. They also used the CrowdRE dataset and got well prepared data from their pre-processing pipeline to work with for their glossary term extraction.

A generally important python library was created in 2010 by {\v R}eh{\r u}{\v r}ek et al. They wanted to automatically create a short list of similar articles to a given article \cite{rehurek_software_2010}. To archive this they used Latent Semantic Analysis, as well as LDA in their approach and created a Python library called \emph{gensim}, which aimed at implementing these techniques in a clear, efficient and scalable way \cite{gensim_python}.