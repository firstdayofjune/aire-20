\section{Conclusion} % (fold)
\label{sec:conclusion}
As mentioned in \autoref{sec:related_work}, the techniques we use were already proven with regards to clustering and topic modeling before. Still, the three approaches deliver results of significantly different quality. In general, we can say that higher detail of our findings comes at the cost of simplicity and speed. Hence it is the third approach especially, which successfully identifies relationships between requirements and clusters them accordingly. Unfortunately, there is no metric by which we rate the quality of the clusters we found. The soft labeling of the data that was done by the users themselves lacks the quality needed for a verification of our clusters. The same applies to the tags, which vary a lot as described in \autoref{sec:own_approach}. The bad quality of the labeling might also result from a missing common understanding of the domains within the crowd workers and form accidentally mis-labeled sentences (as we explain in \autoref{sec:discussion}). To increase the quality of the labeling, manual checking with a lot of effort needs to be performed.

To conclude, our work may form the minimum basis in order to cope with the data challenge DC2 mentioned at the beginning. Our clustering provides some first insights into the dataset, which go beyond what is apparent in the soft-labeling. In the end, still, a lot of manual work needs to be done to find a clear answer to the question, what feature categories are wanted the most, given the crowd sourced data.
% section conclusion (end)