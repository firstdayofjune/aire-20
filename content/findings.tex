\section{Findings} % (fold)
\label{sec:findings}

\begin{itemize}
\item expected 4 clusters and noise for the others
\end{itemize}

\subsection{LDA} % (fold)
\label{sub:findings_lda}

\begin{itemize}
\item found clusters doesn't match the expected tags or domains
\item fast calculation (performing)
\item example for reqs that match but not the domain
\item result: failed
\end{itemize}

\subsection{word2vec} % (fold)
\label{sub:findings_w2v}

\begin{itemize}
\item two clusters -> not matching expected 4 clusters (different domain)
\item fast/performing
\end{itemize}


\subsection{Word Mover's Distance} % (fold)
\label{sub:findings_wmd}

\begin{itemize}
\item best result
\item only 3 clusters instead of expected (4)
\item very long processing time
\item improvement possible because of symmetric matrix
\end{itemize}

\begin{itemize}
\item Conclusion?
\item approaches work better on larger data sets, our data set was way to small
\item soft labeling was done by users
\item cannot be trusted, would have to be checked manually
\item the quality of the labels are bad - matching not as expected
\item common understanding of the domains was not available for the crowd workers, so domains may have been perceived differently
\item In the end, there would still be manual work involved to derive meaningful information with regard to what features or feature categories are wanted the most
\item finally, we can say that crowd re works to some extent: one can gather requirements, but it is very difficult to analyze them automatically or work with them any further 
\end{itemize}

% section analysis (end)