\subsection{Natural Language (Pre-)Processing} % (fold)
\label{sec:nlp}

In order to successfully perform an analysis of the dataset, we first needed to better understand the composition of the data. In a first step we therefore created and analyzed a corpus of requirements. These requirements were formulated as user stories with the defined pattern as described above. Next we compared the results to the Brown Corpus \cite{francis_standard_1965}, a much larger generic corpus with words taken from books and news articles.

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\begin{table*} \centering
\ra{1.3}
\begin{tabular}{  l  l  l  } \toprule
Indicator & \crowdre{} & Brown \\ \midrule
\makecell{Number of Tokens (unique)} & \makecell[c]{90,844 (5,024)} & 1,034,378 \\ 
Number of Lexical Words & 52,266 & 542,924 \\ 
\makecell{Vocabulary Size (Lexical Words)} & 4,906 & 4,6018 \\ 
Vocabulary Size (Stems) & 3,398 & 29,846 \\ 
\makecell{Average Sentence Length (Tokens)} & 31 & 18 \\ 
\makecell{Average Sentence Length (Lexical Words)} & 18 & 10 \\ 
Lexical Diversity & 0.011 & 0.054 \\ \bottomrule
\end{tabular}
\caption{Data from the analysis of the \crowdre{} dataset}\label{tbl-dataset-analysis}
\end{table*}


In \autoref{tbl-dataset-analysis} we can see the number of tokens and lexical words is much larger in the Brown dataset which is a result of a wider variety of words in this kind of texts and is also because the brown dataset contains approximately 10 times more lexical words than the \crowdre{} dataset.
Even though the requirement sentences tend to be much longer, which may have also been caused by the prescribed user story format, the lexical diversity is lower. Requirements use domain-specifc expressions, so the same or similar words appear more often in the written requirements\cite{ferrari_natural_2018}. Additionally the usage of synonyms shall be avoided because is may add ambiguity which is not intended in requirements. In general these results show typical features which are part of a representative dataset that contains only requirements.\\

In order to derive meaningful data from a dataset, we had to perform some Natural Language Processing (NLP) first, before further analyzing the data. A range of NLP techniques exist, which can be used to prepare the data for our kind of analysis\cite{solangi_review_2018}\cite{ferrari_natural_2018}. The following list briefly describes the techniques we used in our research:

\begin{itemize}
	\item \textbf{Tokenization} means separating the text into a sequence of tokens. The tokens are simply the single words that are part of the text. With tokenization, whitespaces and all punctuation is removed from the data. As result a list of tokens is generated. The easiest tokenization is just splitting all alphanumeric characters.
	\item \textbf{Stopword-Removal} is removing common words from the data. They are often only required because of grammar or syntax. These words are not necessary to get the meaning of the text.
	\item \textbf{Stemming} is a technique that reduces a word to just the root of the word. It eliminates duplicates that have the same meaning. This is important in NLP as the conjugation of a word is not important. We are just interested in the semantic information that is contained in the words.
	\item \textbf{Bag-of-Words} is a technique that is used to simplify a sentence or document. The idea is to have a list of all containing words with the corresponding word-count in the text. It therefore only holds the word itself and the multiplicity (or in other words the frequency). It is used to have a numerical representation for the words which can be easier processed by a computer.
	\item \textbf{TF-IDF} can be separated into two different indices. TF: term frequency is the rating how often a specific term occurs in the text. IDF: inverse document frequency is a measure how much information a single term provides in relation to a document. The TF-IDF therefore is a rating how valuable a term for a document is which is represented by the formula: ${TF\text{-}IDF}_{term, document}=TF_{term, document} * log(\frac{N_{documents}}{df_{term}})$.
\end{itemize}