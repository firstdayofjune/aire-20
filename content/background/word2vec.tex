%% Outline:
% - Describe CBOW \cite{mikolov_efficient_2013}
% - Describe skip gram \cite{mikolov_distributed_2013}, \cite{mikolov_efficient_2013}
% - Why were these architectures created? (in the text already)
% - What can you do with these word vectors?
% -- Syntactic similarity (e.g. singular::plural, base::comparative::superlative) \cite{mikolov_linguistic_2013}
% -- Semantic similarity / analogy task (
% 	- France is to Paris as Germany is to Berlin \cite{mikolov_distributed_2013}, see 3 Empirical Results
% 	- What is the word that is similar to small in the same sense as biggest is similar to big? \cite{mikolov_efficient_2013}
% 	- clothing is to shirt as dish is to bowl \cite{mikolov_linguistic_2013}
% 	)
% -- Phrase/Idiom detection \cite{mikolov_distributed_2013}
% - Representing relationships in 2d space
% - Calculation of the cosine distance of these vectors \cite{mikolov_linguistic_2013}
% - The word2vec project
% - Word Embeddings
%
\subsubsection{Word2Vec} % (fold)
\label{sub:word_2_vec}
In 2013, Mikolov et al. proposed two neural network architectures to calculate such vector representations of words: the continuous bag-of-words model (CBOW) and the continuous skip-gram model.




Word2Vec is an open-source project created by Mikolov et al. at Google Inc. for computing vector representations of words and was created by Google Inc. in 2013\footnote{\label{word2vec_link}\url{https://code.google.com/archive/p/word2vec/}, last visited 2020-01-17}. The project incorporates the word2vec tool, which can be used to generate word embeddings from a given text corpus using two neural network architectures - the skip-gram model and the continuous bag-of-words model (CBOW).

Introduced by the same authors, these architectures aimed at optimizing the learning quality of the word vectors, while at the same time reducing the learning time to be able to train the model on data sets with billions of words\cite{mikolov_efficient_2013}. According to their research, "none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words"\cite[p1]{mikolov_efficient_2013} and these architectures (which also includes the previously mentioned LDA) become computationally very expensive with larger data sets. Furthermore, the quality of the learned vectors by previous architectures is inherently limited for their "indifference to word order and their inability to represent idiomatic phrases"\cite[p1]{mikolov_distributed_2013}. This limitation was also important for us to consider during our analysis.\\


As a consequence of the user story format imposed to our requirement sentences a larger number of the requirements contained the phrase "I want my smart home to..." ($416 / 2966 \approx14.03\%$). Also, the requested role description induced some of the participants to start their requirements with "As a smart home owner..." (8 requirements). Even though the latter example may be less relevant in its impact on our findings, it illustrates the problem of idioms just perfectly. Because when calculating the word vectors for these phrases using an LDA, the words "smart", "home" and "owner" would be represented by the same vectors. Hence, the phrase "a smart home owner" would always be represented with the same vectors and the vector distance of this phrase would be similar to both of the phrases "a clever home owner" and "an owner of a smart home". Especially after the stop-words were removed. It is rather obvious though, how these phrases could change the meaning of a statement completely.\\
A combination of two words which appear often together is called a bi-gram (or a 2-gram), but more words than just 2 could be involved making it a combination of an arbitrary number of N words and thus an n-gram. To detect these n-grams usually is done using statistical probability models, where one would analyze a given corpus of words to understand what words frequently occur close to each another\cite{suen_n-gram_1979}. The task of finding n-grams also is not just limited to finding related words, but started as a task of finding related characters which would follow one another\cite{cavnar_n-gram-based_nodate} and was a measurement to improve the performance of automatic character recognition systems\cite{suen_n-gram_1979}.\\
In their word2vec library, Mikolov et al. focused on the application for phrase detection though, to maximize the accuracy on the phrase analogy task. Using a dataset with about 33 billion words, they were able to train a model that reached an accuracy of 72\% for the detection of phrase analogies\cite[p6]{mikolov_distributed_2013}.

\blockcomment{
As discussed earlier, many phrases have a meaning that is not a simple composition of the mean- ings of its individual words. To learn vector representation for phrases, we first find words that appear frequently together, and infrequently in other contexts. For example, “New York Times” and “Toronto Maple Leafs” are replaced by unique tokens in the training data, while a bi-gram “this is” will remain unchanged.
\cite[p5]{mikolov_distributed_2013} 
}