\section{Discussion}
\label{sec:discussion}
As expected, our dataset was probably too small to achieve any better results. In this context, it is important to know how the accuracy of the word2vec phrase detection dropped to 66\% when Mikolov et al. trained their model on a "smaller" dataset of 6 billion words\cite[p7]{mikolov_distributed_2013}. \emph{Smaller} at least in comparison to their final training set, but this is still a lot larger than our dataset by a factor of almost 120.000.

LDA is a typical topic modeling technique, which is proven to work well on large documents. Short texts instead, contain very limited word co-occurrence information. This hinders the LDA to work well on short texts, as we have seen\,\cite{quan_short_2015}.

With our word2vec approach we have to reduce the dimensions of the sentence matrices using PCA and also to reshape the final matrix from 3 to 2 dimensions. As the PCA provides an approximation of the original data, it cannot be avoided that some data is lost in the process\,\cite{wold_principal_1987}. Our word2vec results may therefore be impacted by the dimensionality reduction.

\textit{``The underlying reason is that the document similarity can not be accurately measured under BoW representations due to the extreme sparseness of short texts.''}\,\cite{li_classifying_2019}.

More time would have been needed for the evaluation of our results. Both the tags, as well as the application domains were set by the crowd-workers themselves. The quality of these assignemnts has not been proven yet and we used this data only for lack of proper testing data. For example, one of the requirements with the content "I want my smart home to sync with my biorhythm app and turn on some music that might suit my mood when I arrive home from work so that I can be relaxed" was related to the emph{Entertainment} domain. In our last approach this sentence was found to be in the \emph{Health} domain using the Word Mover's Distance. We could not say that this assignement was definitely wrong, though. So it could be we find our approach a lot more successful after a thorough analysis of all the clusters.

Finally, we lacked prior knowledge of the field of topic modeling and machine learning in general. Though we performed our research with technical and professional care in all conscience and under consideration of commonly accepted principles, there may be a lot of potential for further optimization (which goes beyond changing the hyper-parameters of our models). Future works could be done on an improved set of data, by only analyzing those requirements which have clearly defined domains. Such dataset could be achieved, by cleaning the current \crowdre{} dataset by the means of manually labeling the requirement sentences. This includes verifying the currently assigned application domains, reassigning some domains and also creating new domains, which may not have been in the domain-selection when the requirements were created.\\

When manually reviewing the dataset, our results could also be improved through cleaning the dataset. In accordance with \cite{chu_data_2016} and \cite{krishnan_data_2016}  cleaned datasets have a much higher impact on the training results of ML models than the optimization of hyper parameters. 

- Li et al. also created a classifier using WMD \cite{li_classifying_2019}. Future works could use their approach for a comparison of the generated clusters on the Crowd RE dataset

- \textit{``in general more data (as opposed to simply relevant data) creates better embeddings.''}\,\cite{kusner_word_2015}

- The calculation of the WMD matrix was relatively time consuming. Wu et al. propose a different distance measure for document clustering, which compared to the WMD \textit{``can achieve much lower time complexity with the same accuracy.''}\,\cite{wu_topic_2017}.
\\[2cm]