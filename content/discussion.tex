\section{Discussion}
\label{sec:discussion}
Even though the \crowdre{} dataset is too large to process the requirements manually, it is relatively small for the application of automated topic modeling techniques: LDA is a technique proven to work well on large documents. Short texts instead, contain very limited word co-occurrence information. This hinders the LDA to work well on short texts\,\cite{quan_short_2015}, as we have also seen in our results.\\
The word2vec approach is impacted by the text length as well, since the \textit{``document similarity can not be accurately measured under BoW representations due to the extreme sparseness of short texts.''}\,\cite{li_classifying_2019}. Also, when working with word embeddings, \textit{``in general more data (as opposed to simply relevant data) creates better embeddings.''}\,\cite{kusner_word_2015} As already mentioned, to benchmark word2vec, Mikolov et al. trained their tool on the Google News dataset with 100 billion words, so a dataset 2000 times the size of our dataset. This suggests, better results may be possible using the same techniques on a larger data set.\\
Furthermore, with our word2vec approach we have to reduce the dimensions of the sentence matrices using PCA and also to reshape the final matrix from 3 to 2 dimensions. As the PCA provides an approximation of the original data, it cannot be avoided that some data is lost in the process\,\cite{wold_principal_1987}. Our word2vec results may therefore be additionally impacted by the dimensionality reduction.\\

For a proper evaluation of our results, manual work would be needed. To rate our findings, the dataset has to be labeled properly. For example:

\begin{enumerate}[RE1:]
	\item \textit{``As a home occupant I want music to be played when I get home so that it will help me relax''}\\(\emph{Health})
	\item \textit{``As a home owner I want music to play whenever I am in the kitchen so that I can be entertained while cooking or cleaning''}\\(\emph{Energy})
\end{enumerate}
 
With our word2vec \& WMD approach, these sentences are plotted in close proximity, both located inside the central \emph{Entertainment} cluster in \autoref{fig:wmd-selftrained-1}. We can not say that RE1 is definitely assigned to the wrong domain, we consider a relationship to the \emph{Entertainment} cluster to be equally valid, though. Also, with RE2 the \emph{Energy} domain may have been selected accidentally, as the domains are next to each other in the select box of the form the crowd workers used when they created the requirements. When manually reviewing the dataset to fix the labels, our results could also be improved through generally cleaning the dataset. In accordance with \cite{chu_data_2016} and \cite{krishnan_data_2016} cleaned datasets have a much higher impact on the training results of ML models than the optimization of hyper parameters.\\

Besides cleaning the data base, future work can be done for cross validation and performance improvements: Li et al. also created a classifier using WMD \cite{li_classifying_2019}. Using their approach one could create clusters on the \crowdre{} dataset to compare the findings with our results. Regarding performance improvements, the calculation of the WMD matrix is relatively time consuming. Wu et al. propose a different distance measure for document clustering, which compared to the WMD \textit{``can achieve much lower time complexity with the same accuracy''}\,\cite{wu_topic_2017}. Finally, our work may be used in future attempts to crowd source user requirements for input validation in a web service. When continuously learning and storing the word vectors for new requirements, it would be possible to already suggest similar sentences to the ones a crowd worker is about to enter, based on WMD. If the crowd worker finds his submission to actually overlap with an existing sentence, they could up-vote the existing sentence instead of submitting their sentence. This would not only avoid duplication, but also help in data-driven RE to identify frequently requested requirements without the need for additional data processing.\\[2cm]